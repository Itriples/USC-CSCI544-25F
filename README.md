USC-CSCI544-25F

# 1. LLMs for Time Series Forecasting
## Idea
Use **LLMs** to improve **time series forecasting (TSF)** tasks.

## Related Papers
- [Time-LLM: Time Series Forecasting by Reprogramming Large Language Models](https://arxiv.org/abs/2310.01728)
- [CALF: Aligning LLMs for Time Series Forecasting via Cross-modal Fine-Tuning](https://arxiv.org/abs/2403.07300)

## Datasets
- **ETT** (Electricity Transformer Temperature)
- **Electricity**
- **Traffic** <br>
See datasets [here](https://drive.google.com/drive/folders/1ZOYpTUa82_jCcxIdTmyr0LXQfvaM9vIy)



# 2. Multi-Agent Market Researcher (Applied)
- Agents: one scrapes competitor data, one summarizes market trends, one generates SWOT analysis, one validates findings.
- Use case: Automates consulting-style market research for startups or SMEs.

# 3. 	Recommendation System with Text + Metadata (Applied)
- Combine embeddings from text (e.g., product reviews) with structured data (ratings, price) for hybrid recommendations.
- Great e-commerce angle.

# 4.  Faithful & Source-Grounded Summarization (Research)
- Build a summarization model that not only generates concise summaries but also highlights supporting evidence spans in the source documents.
- Novelty: Tackles the hallucination problem in LLM summarization.
- Reference:
  - Maynez, J., et al. (2020). On Faithfulness and Factuality in Abstractive Summarization. ACL. ğŸ“„
	- Ladhak, F., et al. (2022). Faithful Summarization with Attribution. ACL. ğŸ“„

# 5. Explainable Toxicity Detection (Research)
- Train a toxicity classifier that not only predicts labels but also provides human-interpretable explanations (e.g., highlighting offensive spans).
- Novelty: Bridges fairness + explainability in NLP.
- Reference:
  - Sap, M., et al. (2019). The Risk of Racial Bias in Hate Speech Detection. ACL. ğŸ“„
	- Mathew, B., et al. (2021). Hatexplain: A Benchmark Dataset for Explainable Hate Speech Detection. AAAI



# Multi agent research
Title:
Hierarchical Multi-Agent QA with Specialized Roles and a Verifier for Attribution Reliability

1. Motivation

Large Language Models (LLMs) often hallucinate or provide unsupported claims in complex QA tasks. While multi-agent systems have shown promise, they still lack attribution reliability â€” the ability to ground every answer in verifiable evidence. Recent industry systems (e.g., Anthropicâ€™s Research Assistant with a Citation Agent) highlight this gap, but systematic academic evaluation is missing.

Our project aims to investigate whether combining (a) role specialization (Searcher, Analyst, Writer) and (b) an independent Verifier/Citation Agent under a centralized Orchestrator can significantly improve accuracyâ€“attributionâ€“cost trade-offs in QA tasks.

2. Literature Review

Anthropic (2025). How we built a multi-agent research system.
[Anthropic Blog] â€“ æè¿° Lead Researcher + Citation Agent æ¶æ„ï¼Œå¼ºè°ƒè¯æ®æº¯æºå’Œå®½åº¦ä¼˜å…ˆç ”ç©¶æŸ¥è¯¢ã€‚

Jin et al. (2025). Talk Hierarchically, Act Structurally: Structured Communication Protocols for Multi-Agent LLM Systems. arXiv:2502.11098.
æå‡º structured communication + hierarchical refinementï¼Œæœ‰æ•ˆå‡å°‘åä½œå¹»è§‰ã€‚

Zhang et al. (2025). MACT: Multi-agent Cooperative Tuning for Complex Reasoning with LLMs. NAACL 2025.
ç”¨å¤šä»£ç†åä½œå¤„ç†è¡¨æ ¼é—®ç­”ï¼Œè¯æ˜ plannerâ€“executorâ€“tool ç»“åˆçš„æœ‰æ•ˆæ€§ã€‚

Li et al. (2023). CAMEL: Communicative Agents for â€œMindâ€ Exploration. ICLR 2023.
ç»å…¸å·¥ä½œï¼Œæå‡ºå¤šä»£ç†è§’è‰²æ‰®æ¼”æ¡†æ¶ã€‚

Wu et al. (2023). AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation. Microsoft Research.
å¤šä»£ç†æ¡†æ¶ï¼Œå¹¿æ³›ç”¨äº orchestratorâ€“worker å®ç°ã€‚

Sun et al. (2024). MegaAgent: Scaling LLM-based Multi-Agent Collaboration without Predefined SOPs. arXiv:2408.09955.
æ¢ç´¢ä¸Šç™¾ä»£ç†çš„è‡ªæ²»ä¸åŠ¨æ€ç”Ÿ/æ€ agent ç®¡ç†ã€‚

Zhou et al. (2024). CollabEval: Enhancing LLM-as-a-Judge via Multi-Agent Collaboration. Amazon Science.
æå‡ºä¸‰é˜¶æ®µè¯„ä¼° (åˆè¯„â€“è®¨è®ºâ€“å®šå¤º)ï¼Œæå‡ä¸€è‡´æ€§å’Œé²æ£’æ€§ã€‚

Rashkin et al. (2021). ASQA: Factually Consistent Long-Form Question Answering. NAACL 2021.
æå‡º Attribution-based QA æ•°æ®é›†ï¼Œé€‚åˆè¯„æµ‹ citation precision/recallã€‚

Kamalloo et al. (2023). Evaluating Attributed QA: Can LLMs Reason with Cited Evidence? arXiv:2310.12848.
ç³»ç»Ÿæå‡º Attribution QA æŒ‡æ ‡ï¼ˆCitation F1ã€Attribution Recallï¼‰ã€‚

Manakul et al. (2023). SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models. ACL 2023.
ç”¨å¼‚è´¨é‡‡æ ·æ£€æµ‹ LLM å¹»è§‰ï¼Œå¯ä½œä¸º Verifier è¯„ä»·å‚è€ƒã€‚

Chen et al. (2023). QAFactEval: Improved QA-based Factual Consistency Evaluation for Summarization. EMNLP 2023.
ç”¨äºå¥å­-è¯æ®è•´å«åº¦æ£€éªŒï¼Œå¸¸ç”¨äº QA/æ€»ç»“éªŒè¯ã€‚

Gupta et al. (2022). FEVER: A Large-scale Dataset for Fact Extraction and Verification. NAACL 2022.
äº‹å®æ ¸æŸ¥æ•°æ®é›†ï¼Œæ ‡æ³¨æ”¯æŒ/åé©³/æ— æ³•åˆ¤æ–­ä¸è¯æ®å¥ã€‚

Yang et al. (2018). HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering. EMNLP 2018.
å¤šè·³é—®ç­”æ•°æ®é›†ï¼Œå« supporting factsï¼Œå¯ç”¨äº attribution è¯„æµ‹ã€‚

3. Proposed Method

Architecture:

Orchestrator: decomposes task, assigns roles.

Sub-agents:

Searcher retrieves evidence,

Analyst links evidence to sub-questions,

Writer synthesizes final answer with inline citations.

Verifier/Citation Agent: checks if every statement is properly supported (using AIS/QAFactEval), rejects unsupported outputs â†’ triggers rework.

Structured Protocol: every intermediate/final answer must include evidence slots (document ID + passage).

4. Experiments

Datasets:

HotpotQA (multi-hop QA with supporting facts)

FEVER (fact verification with labeled evidence)

(Optional) ASQA / ELI5-subset (long-form QA with attribution labels)

Metrics:

Accuracy: EM/F1 (HotpotQA), Label Accuracy (FEVER)

Attribution: AIS, Citation Precision/Recall/F1

Cost: tokens, #rounds, latency

Baselines:

Single-agent RAG/ReAct

Multi-agent (no specialization)

Multi-agent (specialization, no verifier)

5. Plan (Milestones)

Week 1â€“2: Literature survey + single-agent baseline (RAG/ReAct).

Week 3â€“4: Implement Orchestrator + specialized Sub-agents; run HotpotQA baseline.

Week 5â€“6: Add Verifier/Citation Agent + structured protocol; full evaluation on HotpotQA + FEVER.

Week 7â€“8: Ablation (remove verifier, remove specialization, remove evidence slots); analyze attributionâ€“accuracyâ€“cost trade-off; finalize report.

6. Expected Contributions

Empirical evidence that Verifier + specialization improves attribution reliability.

Quantitative analysis of accuracyâ€“attributionâ€“cost trade-offs.

Ablation showing which components contribute most (roles, evidence slots, verifier).
